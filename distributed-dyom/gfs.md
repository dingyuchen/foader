# Google File System
[[2021-01-17]]

#school #paper

> Original paper referenced [here](https://pdos.csail.mit.edu/6.824/papers/gfs.pdf)


## Motivation
Usage for Google's distributed file system defers from the traditional assumptions in the following ways:

1. Component failure are the norm rather than the exception.
2. Files are huge by traditional standards
3. Most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically non-existent.
   - Appending is the focus of performance optimization and atomicity guarantees

## Design Overview

### Assumptions
1. System must be able to constantly monitor itself, detect and tolerate and recover from failures on a routine basis.
2. System is optimized for large, multi-GB files.
   - Small files are supported but not optimized for
3. Workload primarily consist of:
   1. Large streaming reads
   2. Small random reads
      - Small reads are batched together as large read by performance conscious applications anyway
4. Workload consist of large, sequential writes
5. System must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.
6. High sustained bandwidth is more important than low latency.

## GFS API
- Support the usual _create_, _delete_, _open_, _close_, _read_, _write_.
- Also provides _snapshot_ and _record append_.
## Architecture
- Single _master_, multiple _chunkservers_.
- Accessed by multiple _clients_.

> Neither the client nor the _chunkservers_ caches file data.

> Client **do** cache metadata

### Chunkserver
Files are divided into fixed-size chunks, each chunk is identified by an immutable and gloablly unique `64-bit` _chunk handle_.
Chunks are replicated on different chunkservers (replication level can be determined by user)

### Single Master
The _master_ 
- maintains all file system metadata
- controls chunk lease management, garbage collection and chunk migration

Polls _chunkservers_ periodically to send instructions and collect state.

#### Shadow Masters
Master state is replicated on other machines in order to achieve fault tolerance and high availability.
Shadow masters lag slightly and may provide stale metadata, however, it allows clients to perform read operations even when the master is down to enhance read availability.

It polls the master for operation log and applies the same mutations to its internal data structure.

### Chunk Size
`64mb` is much larger than typical file system block sizes.

Large chunk size pros:
1. Reduces client's need to interact with the master
2. Reduces network overhead by keeping a persistent connection to the chunkserver
3. Reduces the size of metadata

Cons:
- Small files that consist of only few chunks can become hotspots if many clients are accessing the same file.
  - unlikely since most files are big
  - increase replication factor and stagger batch-queue system.

### Metadata
- File and chunk namespaces
- Mapping from files to chunks
- Locations of each chunk's replicas

Namespaces and mappings are kept persistent by logging mutations to an _operation log_.

Locations are requested from the _chunkservers_ at startup and periodically after. This helps keep the _master_ in sync

### Operation Log
The operation log contains a historical record of critical metadata changes.
- Changes are not made visible to clients until metadata changes are made persistent.
- Log is replicated on multiple remote [machines](#shadow-masters) 
  - A mutation to the state is considered committed only after its log record has been flushed to disk locally and on all master replicas.
- Recovery needs only the latest complete checkpoint and subsequent log files.

## Consistency Model
> A file region is _consistent_ if all clients will always see the same data, regardless of which replicas they read from.

> A region is _defined_ after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.

File namespace mutations (creations) are atomic.

- Singular successful mutation guarantees file region to be defined and consistent.
- Concurrent successful mutations guarantees file region to be undefined but consistent.
- A failed mutation makes the region inconsistent.
  - If a record append fails at any replica, the client retries the operation. GFS only guarantees that the data is written at least once as an atomic unit.
  - Record prepared by the writer contains extra information like checksums so that its validity can be verified. Reader should handle padding and duplicates.

## System Interactions
#tbc

[//begin]: # "Autogenerated link references for markdown compatibility"
[2021-01-17]: ../journal/2021-01-17 "Sunday, January 17, 2021"
[//end]: # "Autogenerated link references"